{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description of the `FinanceNewsScraper` Class\n",
    "\n",
    "The `FinanceNewsScraper` class is designed to scrape financial news articles from the business section of Google News based on a set of specified buzzwords and a given date range.\n",
    "\n",
    "- **Initialization (`__init__`)**: \n",
    "  - The scraper accepts two sets of buzzwords:\n",
    "    - **Must-have buzzwords**: Keywords that must appear in the article title or description.\n",
    "    - **Percentage-based buzzwords**: Keywords that need to match a certain percentage within the article.\n",
    "  - It also takes a `start_date`, `end_date`, and an interval for scraping in chunks (e.g., weekly).\n",
    "\n",
    "- **URL Construction (`construct_url`)**: \n",
    "  - This function builds a Google News RSS URL specifically for the business section, incorporating the provided buzzwords and date range.\n",
    "\n",
    "- **Fetching Data (`fetch_rss_feed`)**: \n",
    "  - This function retrieves the RSS feed using the constructed URL, retrying up to three times if errors are encountered.\n",
    "\n",
    "- **Keyword Matching**:\n",
    "  - **Must-have buzzwords**: Ensures that at least one must-have buzzword appears in the article's title or description.\n",
    "  - **Percentage-based buzzwords**: Verifies that a minimum percentage of the provided buzzwords are present in the article.\n",
    "\n",
    "- **Article Parsing (`parse_articles`)**: \n",
    "  - This function parses the RSS feed and extracts relevant information such as the article title, URL, and publication date, but only for articles that match the buzzword criteria.\n",
    "\n",
    "- **Scraping (`scrape`)**: \n",
    "  - This method iterates through the specified date range, fetching and parsing articles in chunks as defined by the provided interval.\n",
    "\n",
    "- **Saving to CSV (`save_to_csv`)**: \n",
    "  - After scraping, the articles are saved to a CSV file using the `pandas` library for easy storage and further analysis.\n",
    "\n",
    "This class simplifies the process of scraping Google News for business-related articles based on keywords, while also offering functionality to save the results as a CSV file for later analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timedelta\n",
    "import math\n",
    "import time\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinanceNewsScraper:\n",
    "    def __init__(self, primary_buzzwords, secondary_buzzwords, start_date, end_date, required_percentage, interval):\n",
    "        \"\"\"\n",
    "        Initialize the scraper with two sets of buzzwords, start date, end date, and required percentage.\n",
    "        :param primary_buzzwords: List of buzzwords that must be present.\n",
    "        :param secondary_buzzwords: List of buzzwords to search for with percentage matching.\n",
    "        :param start_date: The start date (YYYY-MM-DD) for the articles.\n",
    "        :param end_date: The end date (YYYY-MM-DD) for the articles.\n",
    "        :param required_percentage: The percentage of percentage-based buzzwords that should be present (default 60%).\n",
    "        \"\"\"\n",
    "        self.primary_buzzwords = primary_buzzwords\n",
    "        self.secondary_buzzwords = secondary_buzzwords\n",
    "        self.start_date = datetime.strptime(start_date, '%Y-%m-%d')\n",
    "        self.end_date = datetime.strptime(end_date, '%Y-%m-%d')\n",
    "        self.required_percentage = required_percentage / 100  # Convert percentage to decimal for calculations\n",
    "        self.base_url = \"https://news.google.com/rss\"\n",
    "        self.interval = interval\n",
    "        self.max_retries = 3  # Number of retries in case of failure\n",
    "\n",
    "    def construct_url(self, start_date, end_date):\n",
    "        \"\"\"\n",
    "        Construct the Google News RSS URL with all buzzwords and date range.\n",
    "        :return: The constructed URL.\n",
    "        \"\"\"\n",
    "        combined_buzzwords = self.primary_buzzwords + self.secondary_buzzwords\n",
    "        query = \" AND \".join(combined_buzzwords)  # Combine all buzzwords with 'AND' to ensure all words are present\n",
    "        formatted_query = query.replace(\" \", \"%20\")  # Format query for URL\n",
    "        \n",
    "        url = f\"{self.base_url}?q={formatted_query}+after:{start_date}+before:{end_date}&hl=en-US&gl=US&ceid=US:en\"\n",
    "        \n",
    "        return url\n",
    "\n",
    "    def fetch_rss_feed(self, start_date, end_date):\n",
    "        \"\"\"\n",
    "        Fetch the RSS feed from Google News for a given date range.\n",
    "        :return: BeautifulSoup object with the RSS feed content.\n",
    "        \"\"\"\n",
    "        rss_url = self.construct_url(start_date, end_date)\n",
    "        try:\n",
    "            response = requests.get(rss_url)\n",
    "            if response.status_code == 200:\n",
    "                return BeautifulSoup(response.content, 'xml')  # Parsing as XML\n",
    "            else:\n",
    "                print(f\"Failed to retrieve RSS feed with status code {response.status_code}\")\n",
    "                return None\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error fetching the RSS feed: {e}\")\n",
    "            return None\n",
    "\n",
    "    def contains_any_primary_buzzwords(self, text):\n",
    "        \"\"\"\n",
    "        Check if any must-have buzzwords are present in the given text.\n",
    "        :param text: The text to search for must-have buzzwords (case-insensitive).\n",
    "        :return: True if at least one must-have buzzword is found, False otherwise.\n",
    "        \"\"\"\n",
    "        text = text.lower()\n",
    "        return any(buzzword.lower() in text for buzzword in self.primary_buzzwords)\n",
    "\n",
    "    def contains_percentage_of_buzzwords(self, text):\n",
    "        \"\"\"\n",
    "        Check if at least the required percentage of percentage-based buzzwords are present in the given text.\n",
    "        :param text: The text to search for percentage-based buzzwords (case-insensitive).\n",
    "        :return: True if the required percentage of percentage-based buzzwords are found, False otherwise.\n",
    "        \"\"\"\n",
    "        text = text.lower()\n",
    "        buzzwords_found = sum(1 for buzzword in self.secondary_buzzwords if buzzword.lower() in text)\n",
    "        required_count = math.ceil(len(self.secondary_buzzwords) * self.required_percentage)\n",
    "        \n",
    "        # The condition now checks if at least the required count of buzzwords is found\n",
    "        return buzzwords_found >= required_count\n",
    "\n",
    "    def parse_articles(self, soup):\n",
    "        \"\"\"\n",
    "        Parse the RSS feed and extract article information.\n",
    "        Only return articles where all must-have buzzwords and a percentage of percentage-based buzzwords are found.\n",
    "        :param soup: BeautifulSoup object of the RSS feed.\n",
    "        :return: List of dictionaries with article titles, URLs, and publication dates.\n",
    "        \"\"\"\n",
    "        articles = []\n",
    "        for item in soup.find_all('item'):\n",
    "            title = item.title.text\n",
    "            link = item.link.text\n",
    "            description = item.description.text if item.description else \"\"\n",
    "            pub_date = item.pubDate.text\n",
    "            pub_date = datetime.strptime(pub_date, '%a, %d %b %Y %H:%M:%S %Z')  # Format the date\n",
    "            \n",
    "            # Check if any must-have buzzwords are present in title or description\n",
    "            first_100_words = \" \".join(description.split()[:100])\n",
    "            if self.contains_any_primary_buzzwords(title) or self.contains_any_primary_buzzwords(first_100_words):\n",
    "                # Check if the required percentage of percentage-based buzzwords are present\n",
    "                if self.contains_percentage_of_buzzwords(title) or self.contains_percentage_of_buzzwords(first_100_words):\n",
    "                    articles.append({'title': title, 'url': link, 'date': pub_date})\n",
    "        return articles\n",
    "\n",
    "    def scrape(self):\n",
    "        \"\"\"\n",
    "        Scrape the RSS feed and extract articles that match both must-have and percentage-based buzzwords.\n",
    "        :return: List of articles (titles, URLs, and dates).\n",
    "        \"\"\"\n",
    "        all_articles = []\n",
    "        delta = timedelta(days=self.interval)  # Fetch in intervals (e.g., weekly)\n",
    "        current_start_date = self.start_date\n",
    "        print(f\"Fetching articles from {current_start_date.strftime('%Y-%m-%d')} to {self.end_date.strftime('%Y-%m-%d')}\")\n",
    "\n",
    "        # Loop through the date range with the specified interval\n",
    "        while current_start_date < self.end_date:\n",
    "            current_end_date = min(current_start_date + delta, self.end_date)\n",
    "\n",
    "            soup = self.fetch_rss_feed(current_start_date.strftime('%Y-%m-%d'),\n",
    "                                       current_end_date.strftime('%Y-%m-%d'))\n",
    "            if soup:\n",
    "                articles = self.parse_articles(soup)\n",
    "                all_articles.extend(articles)\n",
    "\n",
    "            current_start_date += delta  # Move to the next interval\n",
    "\n",
    "        if all_articles:\n",
    "            print(f\"Found {len(all_articles)} articles matching the criteria.\")\n",
    "        else:\n",
    "            print(\"No articles found matching the criteria.\")\n",
    "        return all_articles\n",
    "\n",
    "    def save_to_csv(articles, filename):\n",
    "        \"\"\"\n",
    "        Save the scraped articles to a CSV file using pandas.\n",
    "        :param articles: List of articles with title, URL, and date.\n",
    "        :param filename: The name of the CSV file (default is \"articles.csv\").\n",
    "        \"\"\"\n",
    "        # Convert the list of articles to a pandas DataFrame\n",
    "        df = pd.DataFrame(articles)\n",
    "        \n",
    "        # Save DataFrame to CSV\n",
    "        df.to_csv(filename, index=False, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching articles from 2024-10-14 to 2024-10-16\n",
      "Failed to retrieve RSS feed with status code 503\n",
      "Failed to retrieve RSS feed with status code 503\n",
      "No articles found matching the criteria.\n"
     ]
    }
   ],
   "source": [
    "primary_buzzwords = [\" Israel \", \" Gaza \"]\n",
    "secondary_buzzwords = [\" military \", \" humanitarian \", \" escalation \", \" troops \" ]  # List of buzzwords to search for\n",
    "start_date = \"2024-10-14\"  # Start date\n",
    "end_date = \"2024-10-16\"  # End date\n",
    "required_percentage = 25  # required_percentage% of the secondary buzzwords should be in title or description\n",
    "interval = 1\n",
    "\n",
    "scraper = FinanceNewsScraper(primary_buzzwords, secondary_buzzwords, start_date, end_date, required_percentage, interval)\n",
    "articles = scraper.scrape()\n",
    "\n",
    "# Save the output as CSV files \n",
    "FinanceNewsScraper.save_to_csv(articles, \"finance_news.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erdos_fall_2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
