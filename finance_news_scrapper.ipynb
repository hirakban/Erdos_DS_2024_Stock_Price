{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description of the `FinanceNewsScraper` Class\n",
    "\n",
    "The `FinanceNewsScraper` class is designed to scrape financial news articles from the business section of Google News based on a set of specified buzzwords and a given date range.\n",
    "\n",
    "- **Initialization (`__init__`)**: \n",
    "  - The scraper accepts two sets of buzzwords:\n",
    "    - **Must-have buzzwords**: Keywords that must appear in the article title or description.\n",
    "    - **Percentage-based buzzwords**: Keywords that need to match a certain percentage within the article.\n",
    "  - It also takes a `start_date`, `end_date`, and an interval for scraping in chunks (e.g., weekly).\n",
    "\n",
    "- **URL Construction (`construct_url`)**: \n",
    "  - This function builds a Google News RSS URL specifically for the business section, incorporating the provided buzzwords and date range.\n",
    "\n",
    "- **Fetching Data (`fetch_rss_feed`)**: \n",
    "  - This function retrieves the RSS feed using the constructed URL, retrying up to three times if errors are encountered.\n",
    "  - **Robust Retry Mechanism**:\n",
    "      - To ensure stable scraping even when there are network issues, the class includes a retry mechanism. It retries the process multiple times if it fails to retrieve the Yahoo Finance page, adding reliability to the data extraction process.\n",
    "\n",
    "\n",
    "- **Keyword Matching**:\n",
    "  - **Must-have buzzwords**: Ensures that at least one must-have buzzword appears in the article's title or description.\n",
    "  - **Percentage-based buzzwords**: Verifies that a minimum percentage of the provided buzzwords are present in the article.\n",
    "\n",
    "- **Article Parsing (`parse_articles`)**: \n",
    "  - This function parses the RSS feed and extracts relevant information such as the article title, URL, and publication date, but only for articles that match the buzzword criteria.\n",
    "\n",
    "- **Scraping (`scrape`)**: \n",
    "  - This method iterates through the specified date range, fetching and parsing articles in chunks as defined by the provided interval.\n",
    "\n",
    "- **Saving to CSV (`save_to_csv`)**: \n",
    "  - After scraping, the articles are saved to a CSV file using the `pandas` library for easy storage and further analysis.\n",
    "\n",
    "This class simplifies the process of scraping Google News for business-related articles based on keywords, while also offering functionality to save the results as a CSV file for later analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timedelta\n",
    "import math\n",
    "import time\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinanceNewsScraper:\n",
    "    def __init__(self, primary_buzzwords, secondary_buzzwords, start_date, end_date, required_percentage, interval):\n",
    "        \"\"\"\n",
    "        Initialize the scraper with two sets of buzzwords, start date, end date, and required percentage.\n",
    "        :param primary_buzzwords: List of buzzwords that must be present.\n",
    "        :param secondary_buzzwords: List of buzzwords to search for with percentage matching.\n",
    "        :param start_date: The start date (YYYY-MM-DD) for the articles.\n",
    "        :param end_date: The end date (YYYY-MM-DD) for the articles.\n",
    "        :param required_percentage: The percentage of percentage-based buzzwords that should be present (default 60%).\n",
    "        \"\"\"\n",
    "        self.primary_buzzwords = primary_buzzwords\n",
    "        self.secondary_buzzwords = secondary_buzzwords\n",
    "        self.start_date = datetime.strptime(start_date, '%Y-%m-%d')\n",
    "        self.end_date = datetime.strptime(end_date, '%Y-%m-%d')\n",
    "        self.required_percentage = required_percentage / 100  # Convert percentage to decimal for calculations\n",
    "        self.base_url = \"https://news.google.com/rss\"\n",
    "        self.interval = interval\n",
    "        self.max_retries = 3  # Number of retries in case of failure\n",
    "\n",
    "    def construct_url(self, start_date, end_date):\n",
    "        \"\"\"\n",
    "        Construct the Google News RSS URL with all buzzwords and date range.\n",
    "        :return: The constructed URL.\n",
    "        \"\"\"\n",
    "        combined_buzzwords = self.primary_buzzwords + self.secondary_buzzwords\n",
    "        query = \" AND \".join(combined_buzzwords)  # Combine all buzzwords with 'AND' to ensure all words are present\n",
    "        formatted_query = query.replace(\" \", \"%20\")  # Format query for URL\n",
    "        \n",
    "        url = f\"{self.base_url}?q={formatted_query}+after:{start_date}+before:{end_date}&hl=en-US&gl=US&ceid=US:en\"\n",
    "        \n",
    "        return url\n",
    "\n",
    "    def fetch_rss_feed(self, start_date, end_date, max_retries=5, backoff_factor=2):\n",
    "        \"\"\"\n",
    "        Fetch the RSS feed from Google News for a given date range, with retries and exponential backoff to avoid 503 errors.\n",
    "        :param start_date: The start date for fetching articles.\n",
    "        :param end_date: The end date for fetching articles.\n",
    "        :param max_retries: Maximum number of retries if the request fails.\n",
    "        :param backoff_factor: Factor by which the wait time increases after each failure.\n",
    "        :return: BeautifulSoup object with the RSS feed content.\n",
    "        \"\"\"\n",
    "        rss_url = self.construct_url(start_date, end_date)\n",
    "        attempt = 0\n",
    "        delay = 5  # Start with an initial delay of 5 seconds\n",
    "\n",
    "        while attempt < max_retries:\n",
    "            try:\n",
    "                response = requests.get(rss_url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "                \n",
    "                if response.status_code == 200:\n",
    "                    return BeautifulSoup(response.content, 'xml')  # Parsing as XML\n",
    "                else:\n",
    "                    print(f\"Failed to retrieve RSS feed with status code {response.status_code}. Retrying...\")\n",
    "\n",
    "            except requests.RequestException as e:\n",
    "                print(f\"Error fetching the RSS feed: {e}. Retrying...\")\n",
    "\n",
    "            # Apply the exponential backoff\n",
    "            attempt += 1\n",
    "            time.sleep(delay)\n",
    "            delay *= backoff_factor  # Increase the delay exponentially\n",
    "\n",
    "        print(\"Max retries exceeded. Could not fetch the RSS feed.\")\n",
    "        return None\n",
    "\n",
    "\n",
    "    def contains_any_primary_buzzwords(self, text):\n",
    "        \"\"\"\n",
    "        Check if any must-have buzzwords are present in the given text.\n",
    "        :param text: The text to search for must-have buzzwords (case-insensitive).\n",
    "        :return: True if at least one must-have buzzword is found, False otherwise.\n",
    "        \"\"\"\n",
    "        text = text.lower()\n",
    "        return any(buzzword.lower() in text for buzzword in self.primary_buzzwords)\n",
    "\n",
    "    def contains_percentage_of_buzzwords(self, text):\n",
    "        \"\"\"\n",
    "        Check if at least the required percentage of percentage-based buzzwords are present in the given text.\n",
    "        :param text: The text to search for percentage-based buzzwords (case-insensitive).\n",
    "        :return: True if the required percentage of percentage-based buzzwords are found, False otherwise.\n",
    "        \"\"\"\n",
    "        text = text.lower()\n",
    "        buzzwords_found = sum(1 for buzzword in self.secondary_buzzwords if buzzword.lower() in text)\n",
    "        required_count = math.ceil(len(self.secondary_buzzwords) * self.required_percentage)\n",
    "        \n",
    "        # The condition now checks if at least the required count of buzzwords is found\n",
    "        return buzzwords_found >= required_count\n",
    "\n",
    "    def parse_articles(self, soup):\n",
    "        \"\"\"\n",
    "        Parse the RSS feed and extract article information.\n",
    "        Only return articles where all must-have buzzwords and a percentage of percentage-based buzzwords are found.\n",
    "        :param soup: BeautifulSoup object of the RSS feed.\n",
    "        :return: List of dictionaries with article titles, URLs, and publication dates.\n",
    "        \"\"\"\n",
    "        articles = []\n",
    "        for item in soup.find_all('item'):\n",
    "            title = item.title.text\n",
    "            link = item.link.text\n",
    "            description = item.description.text if item.description else \"\"\n",
    "            pub_date = item.pubDate.text\n",
    "            pub_date = datetime.strptime(pub_date, '%a, %d %b %Y %H:%M:%S %Z')  # Format the date\n",
    "            \n",
    "            # Check if any must-have buzzwords are present in title or description\n",
    "            first_100_words = \" \".join(description.split()[:100])\n",
    "            if self.contains_any_primary_buzzwords(title) or self.contains_any_primary_buzzwords(first_100_words):\n",
    "                # Check if the required percentage of percentage-based buzzwords are present\n",
    "                if self.contains_percentage_of_buzzwords(title) or self.contains_percentage_of_buzzwords(first_100_words):\n",
    "                    articles.append({'title': title, 'url': link, 'date': pub_date})\n",
    "        return articles\n",
    "\n",
    "    def scrape(self):\n",
    "        \"\"\"\n",
    "        Scrape the RSS feed and extract articles that match both must-have and percentage-based buzzwords.\n",
    "        :return: List of articles (titles, URLs, and dates).\n",
    "        \"\"\"\n",
    "        all_articles = []\n",
    "        delta = timedelta(days=self.interval)  # Fetch in intervals (e.g., weekly)\n",
    "        current_start_date = self.start_date\n",
    "        print(f\"Fetching articles from {current_start_date.strftime('%Y-%m-%d')} to {self.end_date.strftime('%Y-%m-%d')}\")\n",
    "\n",
    "        # Loop through the date range with the specified interval\n",
    "        while current_start_date < self.end_date:\n",
    "            current_end_date = min(current_start_date + delta, self.end_date)\n",
    "\n",
    "            soup = self.fetch_rss_feed(current_start_date.strftime('%Y-%m-%d'),\n",
    "                                       current_end_date.strftime('%Y-%m-%d'))\n",
    "            if soup:\n",
    "                articles = self.parse_articles(soup)\n",
    "                all_articles.extend(articles)\n",
    "\n",
    "            current_start_date += delta  # Move to the next interval\n",
    "\n",
    "        if all_articles:\n",
    "            print(f\"Found {len(all_articles)} articles matching the criteria.\")\n",
    "        else:\n",
    "            print(\"No articles found matching the criteria.\")\n",
    "        return all_articles\n",
    "\n",
    "    def save_to_csv(articles, filename):\n",
    "        \"\"\"\n",
    "        Save the scraped articles to a CSV file using pandas.\n",
    "        :param articles: List of articles with title, URL, and date.\n",
    "        :param filename: The name of the CSV file (default is \"articles.csv\").\n",
    "        \"\"\"\n",
    "        # Convert the list of articles to a pandas DataFrame and drop duplicates in case there are any\n",
    "        df = pd.DataFrame(articles).drop_duplicates()\n",
    "        \n",
    "        # Save DataFrame to CSV\n",
    "        df.to_csv(filename, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching articles from 2024-09-01 to 2024-10-20\n",
      "Found 13 articles matching the criteria.\n"
     ]
    }
   ],
   "source": [
    "primary_buzzwords = [\"apple\"]\n",
    "secondary_buzzwords = [\"stock\", \"china\", \"strike\"]  # List of buzzwords to search for\n",
    "start_date = \"2024-09-01\"  # Start date\n",
    "end_date = \"2024-10-20\"  # End date\n",
    "required_percentage = 30  # required_percentage% of the secondary buzzwords should be in title or description\n",
    "interval = 1\n",
    "\n",
    "scraper_news = FinanceNewsScraper(primary_buzzwords, secondary_buzzwords, start_date, end_date, required_percentage, interval)\n",
    "articles_news = scraper_news.scrape()\n",
    "\n",
    "# Save the output as CSV files \n",
    "FinanceNewsScraper.save_to_csv(articles_news, \"finance_news.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `FinanceNewsAPIScraper` Class Description\n",
    "\n",
    "The `FinanceNewsAPIScraper` class is designed to fetch, filter, and save news articles from NewsAPI based on specified buzzwords. Key functionalities:\n",
    "\n",
    "- **Initialization**: Takes in an API key, primary and secondary buzzwords, date range, and retry settings.\n",
    "- **Fetch News**: Sends API requests and retries if rate limits are hit.\n",
    "- **Filter**: Filters articles to ensure primary buzzwords are present, with a required percentage of secondary buzzwords.\n",
    "- **Display & Save**: Displays the filtered articles and provides an option to save them to a CSV file.\n",
    "\n",
    "### Key Methods:\n",
    "- `fetch_news()`\n",
    "- `filter_articles()`\n",
    "- `display_articles()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinanceNewsAPIScraper:\n",
    "    def __init__(self, api_key, primary_buzzwords, secondary_buzzwords, start_date, end_date, required_percentage, retry_after):\n",
    "        self.api_key = api_key\n",
    "        self.primary_buzzwords = primary_buzzwords\n",
    "        self.secondary_buzzwords = secondary_buzzwords\n",
    "        self.start_date = start_date\n",
    "        self.end_date = end_date\n",
    "        self.required_percentage = required_percentage / 100\n",
    "        self.base_url = 'https://newsapi.org/v2/everything'\n",
    "        self.retry_after = retry_after\n",
    "\n",
    "    def contains_any_primary_buzzwords(self, text):\n",
    "        text = text.lower()\n",
    "        return any(buzzword.lower() in text for buzzword in self.primary_buzzwords)\n",
    "\n",
    "    def contains_required_percentage_of_secondary_buzzwords(self, text):\n",
    "        text = text.lower()\n",
    "        buzzwords_found = sum(1 for buzzword in self.secondary_buzzwords if buzzword.lower() in text)\n",
    "        required_count = math.ceil(len(self.secondary_buzzwords) * self.required_percentage)\n",
    "        return buzzwords_found >= required_count\n",
    "\n",
    "    def fetch_news(self, retries=3):\n",
    "        params = {\n",
    "                'q': ' OR '.join(self.primary_buzzwords + self.secondary_buzzwords),\n",
    "                'apiKey': self.api_key,\n",
    "                'from': self.start_date,\n",
    "                'to': self.end_date,\n",
    "                'language': 'en',\n",
    "                'sortBy': 'relevancy'\n",
    "            }\n",
    "\n",
    "\n",
    "        attempt = 0\n",
    "        while attempt < retries:\n",
    "            response = requests.get(self.base_url, params=params)\n",
    "            if response.status_code == 200:\n",
    "                return response.json()\n",
    "            elif response.status_code == 429:\n",
    "                print(f\"Rate limit exceeded. Retrying after {self.retry_after} seconds...\")\n",
    "                time.sleep(self.retry_after)\n",
    "            else:\n",
    "                print(f\"Failed to fetch news articles. Status code: {response.status_code}\")\n",
    "                return None\n",
    "            attempt += 1\n",
    "        print(\"Max retries exceeded. Could not fetch the news.\")\n",
    "        return None\n",
    "\n",
    "    def filter_articles(self, news_data):\n",
    "        filtered_articles = []\n",
    "        if news_data and 'articles' in news_data:\n",
    "            for article in news_data['articles']:\n",
    "                title = article['title']\n",
    "                description = article['description'] or \"\"\n",
    "                content = title + \" \" + description\n",
    "                if self.contains_any_primary_buzzwords(content) and self.contains_required_percentage_of_secondary_buzzwords(content):\n",
    "                    filtered_articles.append(article)\n",
    "        return filtered_articles\n",
    "\n",
    "    def display_articles(self, articles):\n",
    "        if articles:\n",
    "            for i, article in enumerate(articles, start=1):\n",
    "                print(f\"{i}. {article['title']} ({article['publishedAt']})\")\n",
    "        else:\n",
    "            print(\"No articles found.\")\n",
    "\n",
    "    def scrape_and_filter_news(self):\n",
    "        news_data = self.fetch_news()\n",
    "        if news_data:\n",
    "            filtered_articles = self.filter_articles(news_data)\n",
    "            self.display_articles(filtered_articles)\n",
    "            return filtered_articles  # Ensure filtered_articles is returned\n",
    "        else:\n",
    "            print(\"Failed to retrieve or filter articles.\")\n",
    "            return []\n",
    "\n",
    "    def save_to_csv(self, articles, filename):\n",
    "        if articles:\n",
    "            data = [{\n",
    "                'title': article['title'],\n",
    "                'publishedAt': article['publishedAt'],\n",
    "                'url': article['url']\n",
    "            } for article in articles]\n",
    "            df = pd.DataFrame(data)\n",
    "            df.to_csv(filename, index=False, encoding='utf-8')\n",
    "            print(f\"Articles saved to {filename}\")\n",
    "        else:\n",
    "            print(\"No articles to save.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_keys = ['51f3c8bce6b1473e9537d03fe37815e3','5d7f3433c9404b6aaba5c5db771f2c79','25d106c70b3c4ff3af1fb174e0afc2ed']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**News Focus:** `Geopoliticial conflicts`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "secondary_buzzwords = [\"israel\", \"gaza\", \"palestine\", \"conflict\", \"war\", \"hamas\", \n",
    "                       \"ukraine\", \"russia\",\"airstrike\",\"attack\", \"crisis\",\"oil\",\"prices\",\"nato\",\"invasion\"\n",
    "                       \"iran\",\"afghanistan\",\"china\",\"taiwan\",\"military\",\n",
    "                       \"indo-pacific\",\"south china sea\",\"market\",\"nuclear\",\"escalate\",\"zelensky\",\"putin\"]  # List of buzzwords to search for\n",
    "required_percentage = 6\n",
    "retry_after = 60\n",
    "start_date = '2024-09-23'\n",
    "end_date = '2024-10-22'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erdos_fall_2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
